{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de24ec43-393b-4a7a-8832-70a8a174cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1516807b-f2f2-4dfc-8ef8-0169ce8c7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if a word pair is present in the page. Input: Page text as a list and one pair of words. Output: True or False\n",
    "def find_word(word_list, pair1):\n",
    "    pair1_found = False\n",
    "\n",
    "    for i in range(len(word_list) - 1):\n",
    "        if word_list[i] == pair1[0] and word_list[i + 1] == pair1[1]:\n",
    "            pair1_found = True\n",
    "\n",
    "    return pair1_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec05c1a-1cf2-4207-beba-ff1924e7ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if two word pairs are present in the page. Input: Page text as a list and two pairs of words. Output: True or False\n",
    "def find_word_pairs(word_list, pair1, pair2):\n",
    "    pair1_found = False\n",
    "    pair2_found = False\n",
    "    \n",
    "    # Convert all words and pairs to lowercase for case-insensitive comparison\n",
    "    word_list_lower = [item.lower() for item in word_list]\n",
    "    pair1_lower = (pair1[0].lower(), pair1[1].lower())\n",
    "    pair2_lower = (pair2[0].lower(), pair2[1].lower())\n",
    "    \n",
    "    for item in word_list_lower:\n",
    "        if item.startswith(pair1_lower[0]) and any(word.startswith(pair1_lower[1]) for word in word_list_lower):\n",
    "            pair1_found = True\n",
    "        if item.startswith(pair2_lower[0]) and any(word.startswith(pair2_lower[1]) for word in word_list_lower):\n",
    "            pair2_found = True\n",
    "        \n",
    "    return pair1_found, pair2_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4842c1-1870-47ac-974a-3d731e729f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge -ve sign which are split from value. Input: Page text as a list. Output: Page text as a list after attaching -ve sign with following numeric value\n",
    "def merge_dash(data):\n",
    "    result = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        if data[i] == '-' and i + 1 < len(data) and re.match(r'^[0-9\\.]', data[i + 1]):\n",
    "            result.append(data[i] + data[i + 1])\n",
    "            skip_next = True\n",
    "        else:\n",
    "            result.append(data[i])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d0de66-f1ea-4557-b43b-297457f74c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge single digit (not 0) which are split from numeric values. Input: Page text as a list. Output: Page text as a list after attaching single digit with preceding numeric values. \n",
    "def merge_single_digit(data):\n",
    "    output = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        if i < len(data) - 1 and data[i + 1].isdigit() and len(data[i + 1]) == 1 and data[i + 1] != '0':\n",
    "            if data[i].replace('.', '', 1).isdigit() or 'E' in data[i]:\n",
    "                output.append(data[i] + data[i + 1])\n",
    "            else:\n",
    "                output.append(data[i])\n",
    "                output.append(data[i + 1])\n",
    "            i += 2\n",
    "        else:\n",
    "            output.append(data[i])\n",
    "            i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4cf757-61e5-4ecc-af34-a6935fb27ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge numerical values that are split at \"-\", \"+\" or \"E\". Input: Page text as a list. Output: Page text as a list after attaching numeric values that are split. \n",
    "def merge_numericals(data):\n",
    "    merged_data = []\n",
    "    for sublist in data:\n",
    "        merged_sublist = []\n",
    "        skip_next = False\n",
    "        for i in range(len(sublist) - 1):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                continue\n",
    "            if (sublist[i].endswith((\"-\", \"+\", \"E\"))) and (sublist[i+1][0].isdigit()):\n",
    "                merged_sublist.append(sublist[i] + sublist[i+1])\n",
    "                skip_next = True\n",
    "            else:\n",
    "                merged_sublist.append(sublist[i])\n",
    "        if not skip_next:\n",
    "            merged_sublist.append(sublist[-1])  # Add the last element if it wasn't merged\n",
    "        merged_data.append(merged_sublist)\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5e79f1-64a5-487a-b325-87a38117be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if a value is numeric. Input: A string element from a list. Output: True or False\n",
    "def is_numeric(s):\n",
    "    try:\n",
    "        # Replace commas with periods to handle decimal and scientific notation\n",
    "        s = s.replace(',', '.')\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a27c384-a4ae-4f2a-b205-31ebdd5a8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge superscript/Subscript with indicator. Input: Page text as a list. Output: Page text as a list after attaching super/sub script with indicator.\n",
    "def attach_single_digit(lines):\n",
    "    for sublist in lines:\n",
    "        for i in range(1, len(sublist)):\n",
    "            if sublist[i].isdigit() and len(sublist[i]) == 1 and sublist[i] != '0':\n",
    "                sublist[i - 1] += sublist[i]\n",
    "                sublist.pop(i)\n",
    "                break\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e78695a9-22bc-4411-af7f-f7d457287821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the table as a list handling empty spaces, split -ve values and split single digits. Input: Document, Page number and start_keyword. Output: Table as a list with no empty spaces, no split -ve values and no split single digits.   \n",
    "def extract_text_from_page(document, page_num, start_keyword):\n",
    "    page = document.load_page(page_num)\n",
    "    text = page.get_text()\n",
    "    sections = re.split(r'\\n\\n+', text.strip())\n",
    "    lines = sections[0].split('\\n')\n",
    "    \n",
    "    # Update to check if the line starts with the start_keyword\n",
    "    pos = next((i for i, line in enumerate(lines) if line.strip().lower().startswith(start_keyword.lower())), None)\n",
    "    \n",
    "    if pos is not None:\n",
    "        lines = lines[pos:]\n",
    "        \n",
    "    lines = [val for val in lines if val.strip()]\n",
    "    lines = [item for sublist in [element.split() for element in lines] for item in sublist]\n",
    "    lines = merge_dash(lines)\n",
    "    lines = merge_single_digit(lines)\n",
    "  \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b831085-6e01-4b49-8f0a-4cf047cf6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the table into rows (Attach indicators and group them with their 15 values). Input: Table as a list with split indicators. Output: List of lists, where each list is a row in the table. \n",
    "def parse_lines_to_result(lines, ind_rows, standard_rows):\n",
    "    result = []\n",
    "    temp = []\n",
    "\n",
    "    # Attach indicators by attaching non numeric elements\n",
    "    for item in lines:\n",
    "        if is_numeric(item):\n",
    "            if len(item) == 1 and item != '0':  # Check if the numeric item is a single digit and not 0\n",
    "                temp.append(item)  # Include it in the current non-numeric group\n",
    "            else:\n",
    "                if temp:\n",
    "                    result.append(\" \".join(temp))\n",
    "                    temp = []\n",
    "                result.append(item)\n",
    "        else:\n",
    "            temp.append(item)\n",
    "    \n",
    "    # If there are any remaining items in temp, join and add them to the result\n",
    "    if temp:\n",
    "        result.append(\" \".join(temp))\n",
    "  \n",
    "    #Standardize the indicators using master dictionary\n",
    "    for i in range(len(result)):\n",
    "        for standard_row in standard_rows:\n",
    "            if result[i] in standard_row:\n",
    "                result[i] = standard_row[0]\n",
    "                break\n",
    "\n",
    "    #Group indicator and 15 values into rows\n",
    "    rows = []\n",
    "    i = 0\n",
    "    while i < len(result):\n",
    "        \n",
    "        current_item = result[i]\n",
    "        \n",
    "        if any(current_item.lower().startswith(row.lower()) for row in ind_rows):\n",
    "            row = []\n",
    "            for j in range(i, i + 16):\n",
    "                if j < len(result):\n",
    "                    item = result[j]\n",
    "                    \n",
    "                    #Check if any of the item is an indicator which means 15 values are not available\n",
    "                    if any(item.lower().startswith(ind_row.lower()) for ind_row in ind_rows) and j > i:\n",
    "                        break\n",
    "                    row.append(item)\n",
    "                else:\n",
    "                    row.append('0')\n",
    "            # Fill the remaining items with '0' to make the row length 16\n",
    "            row.extend(['0'] * (16 - len(row)))\n",
    "            rows.append(row)\n",
    "            i = j  # Update the index to the last checked position\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "649f1c05-a6cd-42c8-90c1-3947b323e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract values between two keywords from a list. Input: Page text as a list, start and end keyword. Output: String that attached every element between start and end keywords\n",
    "def extract_values_between_keywords(lines, start_keyword, end_keyword):\n",
    "    start_idx = -1\n",
    "    end_idx = -1\n",
    "\n",
    "    # Find the start index\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().endswith(start_keyword):\n",
    "            start_idx = i + 1\n",
    "            break\n",
    "\n",
    "    # Find the end index\n",
    "    for i, line in enumerate(lines[start_idx:], start=start_idx):\n",
    "        if line.strip().startswith(end_keyword):\n",
    "            end_idx = i\n",
    "            break\n",
    "\n",
    "    # Extract and return the values between the keywords\n",
    "    if start_idx != -1 and end_idx != -1:\n",
    "        return \" \".join(lines[start_idx:end_idx]).strip()\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cdb06e1-2e62-4bbb-9508-167410723013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Handle exception case): Format product name where it is stored as image. Input: Product name without \"Orea\" (image) . Output: \"Orea\" added to product name\n",
    "def add_orea_to_thickness(prod_name):\n",
    "    # Regular expression to match if the product name starts with thickness (e.g., '10 mm')\n",
    "    match = re.match(r'(\\d+\\s*mm)(.*)', prod_name, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        thickness = match.group(1).strip()\n",
    "        rest_of_name = match.group(2).strip()\n",
    "        # Add 'Orea' before the thickness\n",
    "        return f'Orea {thickness} {rest_of_name}'\n",
    "    else:\n",
    "        return prod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f352a9a-285c-46e7-af63-5f7cb24eee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise all the variables.\n",
    "#Standard rows contains a list of lists where each list contains all the variations of a particular indicator.\n",
    "#env_rows, resource_rows, waste_rows are the variations or particular indiactor for that specific table.\n",
    "#Two pairs of keywords for locating the table pages. \n",
    "# Columns for each table\n",
    "#Keyword to locate the start of each table. \n",
    "\n",
    "file_path = 'Dictionary.xlsx'  \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Convert each column to a list including the header\n",
    "data_with_headers = [[col] + df[col].tolist() for col in df.columns]\n",
    "\n",
    "standard_rows = [[x for x in sublist if not (pd.isna(x) or (isinstance(x, float) and math.isnan(x)))] for sublist in data_with_headers]\n",
    "\n",
    "standard_rows = standard_rows[1::]\n",
    "\n",
    "env_rows = standard_rows[:13]\n",
    "resource_rows = standard_rows[13:23]\n",
    "waste_rows = standard_rows[23:]\n",
    "\n",
    "env_rows = [item for sublist in env_rows for item in sublist]\n",
    "resource_rows = [item for sublist in resource_rows for item in sublist]\n",
    "waste_rows = [item for sublist in waste_rows for item in sublist]\n",
    "\n",
    "\n",
    "# Parameters for Environmental Impacts\n",
    "env_impacts_pair1 = (\"environmental\", \"impacts\")\n",
    "env_impacts_pair2 = (\"climate\", \"change\")\n",
    "env_impacts_columns = ['Environmental indicators', 'A1 / A2 / A3', 'A4 Transport', 'A5 Installation', 'B1 Use', 'B2 Maintenance', 'B3 Repair', 'B4 Replacement', 'B5 Refurbishment', 'B6 Operational energy use', 'B7 Operational water use', 'C1 Deconstruction / demolition', 'C2 Transport', 'C3 Waste processing', 'C4 Disposal', 'D Reuse, recovery, recycling']\n",
    "env_impacts_start_keyword = \"Climate Change\"\n",
    "env_impacts_num_columns = len(env_impacts_columns)\n",
    "\n",
    "# Parameters for Resource Use\n",
    "resource_use_pair1 = (\"resources\", \"use\")\n",
    "resource_use_pair2 = (\"primary\", \"energy\")\n",
    "resource_use_columns = ['Resoruces Use Indicators', 'A1 / A2 / A3', 'A4 Transport', 'A5 Installation', 'B1 Use', 'B2 Maintenance', 'B3 Repair', 'B4 Replacement', 'B5 Refurbishment', 'B6 Operational energy use', 'B7 Operational water use', 'C1 Deconstruction / demolition', 'C2 Transport', 'C3 Waste processing', 'C4 Disposal', 'D Reuse, recovery, recycling']\n",
    "resource_use_start_keyword = \"Use of renewable primary energy\"\n",
    "resource_use_num_columns = len(resource_use_columns)\n",
    "\n",
    "# Parameters for Waste Output Flows\n",
    "waste_pair1 = (\"waste\", \"category\")\n",
    "waste_pair2 = (\"hazardous\", \"waste\")\n",
    "waste_columns = ['Waste Category & Output Flows', 'A1 / A2 / A3', 'A4 Transport', 'A5 Installation', 'B1 Use', 'B2 Maintenance', 'B3 Repair', 'B4 Replacement', 'B5 Refurbishment', 'B6 Operational energy use', 'B7 Operational water use', 'C1 Deconstruction / demolition', 'C2 Transport', 'C3 Waste processing', 'C4 Disposal', 'D Reuse, recovery, recycling']\n",
    "waste_start_keyword = \"Hazardous waste disposed\"\n",
    "waste_num_columns = len(waste_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9995877a-7682-4786-9045-4b106522baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Other Details'], ['Valid Until:', 'Date of validity:', 'Valid until:'], ['Date of publication:', 'Date of issue:', 'Publication Date:', 'Original issue:'], ['FUNCTIONAL UNIT', 'DECLARED UNIT'], ['Scope of the EPD'], ['Third party verifier', 'Third-party verifier'], ['SYSTEM '], ['REFERENCE '], ['CUT-OFF RULES'], ['EPD Type'], ['Standards'], ['Site of manufacture', 'Production plant'], ['Management system', 'Management system', 'Owner', 'Programme used'], ['Product Name']]\n"
     ]
    }
   ],
   "source": [
    "#Extract the variations of keywords for extracting metadata (other_details)\n",
    "\n",
    "file_path = r\"C:\\Users\\S2878088\\OneDrive - Saint-Gobain\\Downloads\\SMART\\Dictionary.xlsx\" \n",
    "df = pd.read_excel(file_path, sheet_name=1)\n",
    "\n",
    "# Convert each column to a list including the header\n",
    "data_with_headers = [[col] + df[col].tolist() for col in df.columns]\n",
    "\n",
    "standard_details = [[x for x in sublist if not (pd.isna(x) or (isinstance(x, float) and math.isnan(x)))] for sublist in data_with_headers]\n",
    "\n",
    "# Output the result\n",
    "print(standard_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "820fccec-af61-489f-8405-beccdcc8f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the table as a DataFrame. \n",
    "#Input: The file path, keyword pairs to locate the table page, column names, the keyword to locate the start of the table, the number of columns, list of different variations of keywords of that particular table, list of lists different variations of keywords of all tables.\n",
    "#Output: The particular table as a DataFrame\n",
    "def extract_data_from_pdf(file_path, pair1, pair2, column_names, start_keyword, num_columns, ind_rows, standard_rows):\n",
    "    document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        temp_text = page.get_text()\n",
    "        pair1_found, pair2_found = find_word_pairs(temp_text.split(), pair1, pair2)\n",
    "        if pair1_found and pair2_found:\n",
    "            text = temp_text\n",
    "            break\n",
    "            \n",
    "    if not text:\n",
    "        raise ValueError(\"Text with the required pairs not found in the PDF.\")\n",
    "    \n",
    "    lines = extract_text_from_page(document, page_num, start_keyword)\n",
    "\n",
    "    lines = [lines]\n",
    "    \n",
    "    lines = merge_numericals(lines)\n",
    "    \n",
    "    lines = attach_single_digit(lines)\n",
    "    \n",
    "    result = parse_lines_to_result(lines[0], ind_rows, standard_rows)\n",
    "    \n",
    "    tot_elements = sum(len(row) for row in result)\n",
    "    \n",
    "    if start_keyword.startswith(\"Climate Change\"):\n",
    "        no_elements = 208\n",
    "    elif start_keyword.startswith(\"Use of renewable primary energy\"):\n",
    "        no_elements = 160\n",
    "    else:\n",
    "        no_elements = 128\n",
    "        \n",
    "    remaining = no_elements - tot_elements\n",
    "    \n",
    "    if remaining > 0:\n",
    "        t_lines = extract_text_from_page(document, page_num + 1, start_keyword)\n",
    "        t_lines = [t_lines]\n",
    "        \n",
    "        t_result = parse_lines_to_result(t_lines[0], ind_rows, standard_rows)\n",
    "        result += t_result\n",
    "    return pd.DataFrame(result, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ae7dca-7e16-4472-9e5b-0d3d44916aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract metadata as a dictionary (Everything other than the tables). \n",
    "#Input: The file path\n",
    "#Output: Dictionary of metadata details\n",
    "def extract_other_details_from_pdf(file_path):\n",
    "    document = fitz.open(file_path)\n",
    "    \n",
    "#----- Scope -----\n",
    "    \n",
    "    page = document.load_page(0)\n",
    "    text = page.get_text()\n",
    "    sections = re.split(r'\\n\\n+', text.strip())\n",
    "    lines = sections[0].split('\\n')\n",
    "\n",
    "    pos = next((i for i, line in enumerate(lines) if line.startswith(\"Scope of the EPD\")), None)\n",
    "    if pos is not None:\n",
    "        scope_lines = lines[pos:]\n",
    "\n",
    "    scope_element = next((item for item in lines if item.startswith('Scope of the EPD')), None)\n",
    "    \n",
    "    if scope_element:\n",
    "        # Remove the 'Scope of the EPD' part\n",
    "        scope = re.sub(r'Scope of the EPD(?:®)?: ', '', scope_element).strip()\n",
    "        \n",
    "        if scope == \"\" or scope == \" \":\n",
    "            scope = scope_lines[1]\n",
    "            \n",
    "        # If 'scope' ends with 'and', find the next word in the lines list and append it\n",
    "        elif scope.endswith('and'):\n",
    "            # Find the index of the scope_element in lines\n",
    "            scope_index = lines.index(scope_element)\n",
    "            \n",
    "            # Check if there is a next line and append it to the scope\n",
    "            if scope_index + 1 < len(lines):\n",
    "                next_word = lines[scope_index + 1].strip()\n",
    "                scope = f\"{scope} {next_word}\"\n",
    "\n",
    "    else:\n",
    "        scope = \"Not found\"\n",
    "\n",
    "\n",
    "#----- Product Name -----\n",
    "    \n",
    "    if scope.startswith(\"Europe\"):\n",
    "        prod_name = extract_values_between_keywords(lines, 'EN 15804:2012+A2:2019 for:', 'Version')\n",
    "        prod_name = add_orea_to_thickness(prod_name)\n",
    "        if prod_name == '':\n",
    "            prod_name = lines[0]+\"Orae \"+lines[-1]\n",
    "\n",
    "    else:\n",
    "        pattern = re.compile(r'EN 15804:2012\\+A2:2019(?:/AC[: ]?2021)?')\n",
    "    \n",
    "        # Get starting index\n",
    "        start_index = next((i for i, s in enumerate(lines) if pattern.search(s)), -1) + 1\n",
    "        end_index = next(\n",
    "        (index for index, item in enumerate(lines) if item.startswith('Version') or item.startswith('Date') or item.startswith('Manufacturer') or item.startswith('VERSION')),\n",
    "        None)\n",
    "        \n",
    "        if start_index is not None and end_index is not None:\n",
    "            prod_name = ''.join(lines[start_index:end_index]).strip()\n",
    "        else:\n",
    "            prod_name = \"Not found\"\n",
    "\n",
    "    \n",
    "#----- Valid Until -----\n",
    "\n",
    "    #Load the Valid Until Keywords from the master excel file in sheet 2 \n",
    "    valid_until_phrases = standard_details[1]\n",
    "    \n",
    "    valid_until_element = next(\n",
    "        (item for item in lines if any(item.startswith(phrase) for phrase in valid_until_phrases)), \n",
    "        None )\n",
    "    \n",
    "    valid_until = valid_until_element.split(\":\", 1)[1].strip()\n",
    "    \n",
    "    if valid_until == \"\" or valid_until == \" \":\n",
    "        #Valid_until element is split from description, ex: \"Valid Unitl:\", \"20-04-30\" \n",
    "        if valid_until_element is not None:\n",
    "            index = lines.index(valid_until_element)\n",
    "            \n",
    "            # Ensure there's an element after the found element\n",
    "            if index + 1 < len(lines):\n",
    "                valid_until = lines[index + 1]\n",
    "            else:\n",
    "                valid_until = \"Not Found\"\n",
    "        else:\n",
    "            valid_until = \"Not Found\"\n",
    "    \n",
    "\n",
    "#----- Date of Publication -----\n",
    "    \n",
    "    date_of_publication_phrases = standard_details[2]\n",
    "    \n",
    "    date_of_publication_element = next(\n",
    "        (item for item in lines if any(item.startswith(phrase) for phrase in date_of_publication_phrases)), \n",
    "        None )\n",
    "\n",
    "    \n",
    "    # Remove the prefix and strip whitespace if the element is found\n",
    "    if date_of_publication_element:\n",
    "        date_of_publication = date_of_publication_element.split(\":\", 1)[1].strip()\n",
    "    else:\n",
    "        date_of_publication = \"Not found\"\n",
    "\n",
    "\n",
    "    if date_of_publication == \"\" or date_of_publication == \" \":\n",
    "        if date_of_publication_element is not None:\n",
    "            index = lines.index(date_of_publication_element)\n",
    "            \n",
    "            # Ensure there's an element after the found element\n",
    "            if index + 1 < len(lines):\n",
    "                date_of_publication = lines[index + 1]\n",
    "            else:\n",
    "                date_of_publication = \"Not found\"\n",
    "        else:\n",
    "            date_of_publication = \"Not found\"\n",
    "\n",
    "    \n",
    "#----- Third Party-----    \n",
    "    \n",
    "    verfiy_name1 = (\"Geographical\", \"scope\")\n",
    "    verify_name2 = (\"EPD®\", \"registration\")\n",
    "    document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        temp_text = page.get_text()\n",
    "        pair1_found, pair2_found = find_word_pairs(temp_text.split(), verfiy_name1, verify_name2)\n",
    "        if pair1_found and pair2_found:\n",
    "            text = temp_text\n",
    "            break\n",
    "    \n",
    "    sections = re.split(r'\\n\\n+', text.strip())\n",
    "    lines = sections[0].split('\\n')\n",
    "    \n",
    "    third_party_phrases = standard_details[5]\n",
    "    # Check if any element contains \"Third party verifier\"\n",
    "    third_party = any(phrase in item for item in lines for phrase in third_party_phrases)\n",
    "    \n",
    "    if third_party:\n",
    "        third_party = \"Verified\"\n",
    "    else:\n",
    "        third_party = \"Not Verified\"\n",
    "\n",
    "\n",
    "#----- Site of Manufacture -----\n",
    "    \n",
    "    site_keyword_start = standard_details[-3]\n",
    "    site_keyword_end = standard_details[-2]\n",
    "    \n",
    "    start_index = next((i for i, item in enumerate(lines) if any(item.startswith(keyword) for keyword in site_keyword_start)), None)\n",
    "    rem_lines = lines[start_index:]\n",
    "    end_index = next((i for i, item in enumerate(rem_lines) if any(item.startswith(keyword) for keyword in site_keyword_end)), None)\n",
    " \n",
    "\n",
    "    if start_index is not None and end_index is not None:\n",
    "        production_plant = ' '.join(rem_lines[0:end_index]).strip()\n",
    "        production_plant = production_plant.split(':', 1)[1].strip()\n",
    "    else:\n",
    "        production_plant = \"Not found\"\n",
    "\n",
    "    \n",
    "#----- Functional Unit -----\n",
    "    \n",
    "    functional_unit_pair = (\"SYSTEM\", \"BOUNDARIES\")\n",
    "    document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        temp_text = page.get_text()\n",
    "        pair1_found = find_word(temp_text.split(), functional_unit_pair)\n",
    "        if pair1_found:\n",
    "            text = temp_text\n",
    "            break\n",
    "    \n",
    "    sections = re.split(r'\\n\\n+', text.strip())\n",
    "    lines = sections[0].split('\\n')\n",
    "    normalized_data = [item.strip() for item in lines]\n",
    "\n",
    "    functional_unit_phrases = standard_details[3]\n",
    "    system_boundary_phrases = standard_details[6]\n",
    "    reference_service_life_phrases = standard_details[7]\n",
    "    cut_off_phrases = standard_details[8]\n",
    "\n",
    "   \n",
    "    # Extract Functional Unit\n",
    "    start_index = next((i for i, item in enumerate(normalized_data) if any(item.startswith(phrase) for phrase in functional_unit_phrases)), None)\n",
    "    end_index = next((i for i, item in enumerate(normalized_data) if any(item.startswith(phrase) for phrase in  system_boundary_phrases)), None)\n",
    "    \n",
    "    # Join lines between start_index and end_index if both are found\n",
    "    if start_index is not None and end_index is not None:\n",
    "        \n",
    "        functional_unit = ' '.join(normalized_data[start_index:end_index])\n",
    "\n",
    "        # Remove each phrase from functional_unit\n",
    "        for phrase in functional_unit_phrases:\n",
    "            functional_unit = functional_unit.replace(phrase, '')\n",
    "        \n",
    "        # Strip any extra spaces after replacements\n",
    "        functional_unit = functional_unit.strip()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        functional_unit = \"Not found\"\n",
    "    \n",
    "    \n",
    "    # Extract System Boundaries\n",
    "    start_index = next((i for i, item in enumerate(normalized_data) if any(item.startswith(phrase) for phrase in system_boundary_phrases)), None)\n",
    "    end_index = next((i for i, item in enumerate(normalized_data) if any(item.startswith(phrase) for phrase in  reference_service_life_phrases)), None)\n",
    "    system_boundaries = ' '.join(item.strip() for item in lines[start_index + 1:end_index]) if start_index and end_index else \"Not found\"\n",
    "    system_boundaries = system_boundaries.replace('SYSTEM', '').replace('BOUNDARIES', '').strip()\n",
    "    \n",
    "    # Extract Reference Service Life\n",
    "    start_index = next((i for i, item in enumerate(normalized_data) if any(item.startswith(phrase) for phrase in reference_service_life_phrases)), None)\n",
    "    end_index = next((i for i, item in enumerate(normalized_data) if any(item.startswith(phrase) for phrase in  cut_off_phrases)), None)\n",
    "    rsl = ' '.join(item.strip() for item in lines[start_index + 1:end_index]) if start_index and end_index else \"Not found\"\n",
    "    match = re.search(r'(\\d+)\\s+years', rsl)\n",
    "    rsl = match.group(1) if match else \"Not found\"\n",
    "    \n",
    "    \n",
    "    return  {\n",
    "        \"EPD Type\": \"Product Specific\",\n",
    "        \"Third Party\": third_party,\n",
    "        \"Geographical Scope\": scope,\n",
    "        \"Standards\": \"EN 15804 and ISO 21930\",\n",
    "        \"life_cycle_stages\" : [\"A1 / A2 / A3\", \"A4 Transport\", \"A5 Installation\", \"B1 Use\", \"B2 Maintenance\", \"B3 Repair\", \"B4 Replacement\", \"B5 Refurbishment\", \"B6 Operational energy use\", \"B7 Operational water use\", \"C1 Deconstruction / demolition\", \"C2 Transport\", \"C3 Waste processing\", \"C4 Disposal\", \"D Reuse, recovery, recycling\"],\n",
    "        \"Functional Unit\": functional_unit,\n",
    "        \"Reference Service Life\": rsl,\n",
    "        \"System Boundaries\": system_boundaries,\n",
    "        \"Date of Publication\": date_of_publication,\n",
    "        \"Site of Manufacture\": production_plant,\n",
    "        \"Product Name\": prod_name,\n",
    "        \"Valid Until\": valid_until,\n",
    "    }\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a31240-dc25-4aa6-8a35-3d44b00bfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process (Concatenate all tables, add metadata as columns and do all the data formatting) all the data together and store it in an \n",
    "#  file.\n",
    "#Input: The three tables as DataFrame, dictionary containing metadata detials and name of the pdf. \n",
    "#Output: Store the data of EPD (the tables and metadata) in an excel in the 'Excel' directory.\n",
    "def process_and_save_data(env_impacts_df, resource_use_df, waste_impacts_df, other_details, pdf, output_path='Excel'):\n",
    "    # Update column names for each DataFrame\n",
    "    env_impacts_df.columns = ['Indicators'] + env_impacts_df.columns[1:].tolist()\n",
    "    resource_use_df.columns = ['Indicators'] + resource_use_df.columns[1:].tolist()\n",
    "    waste_impacts_df.columns = ['Indicators'] + waste_impacts_df.columns[1:].tolist()\n",
    "\n",
    "    # Concatenate the DataFrames vertically\n",
    "    combined_df = pd.concat([env_impacts_df, resource_use_df, waste_impacts_df], axis=0)\n",
    "\n",
    "\n",
    "    # Add each detail as a new column to combined_df, filled with the corresponding value\n",
    "    for detail, value in other_details.items():\n",
    "        combined_df[detail] = [value] * len(combined_df)\n",
    "\n",
    "   # Parse dates with multiple formats\n",
    "    combined_df['Date of Publication'] = pd.to_datetime(combined_df['Date of Publication'], errors='coerce', dayfirst=True)\n",
    "    combined_df['Valid Until'] = pd.to_datetime(combined_df['Valid Until'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "    # Convert to DD-MM-YYYY format\n",
    "    combined_df['Date of Publication'] = combined_df['Date of Publication'].dt.strftime('%d-%m-%Y')\n",
    "    combined_df['Valid Until'] = combined_df['Valid Until'].dt.strftime('%d-%m-%Y')\n",
    "    \n",
    "    # List of columns to be converted to floating-point numbers\n",
    "    columns_to_convert = ['A1 / A2 / A3', 'A4 Transport', 'A5 Installation', 'B1 Use', 'B2 Maintenance',\n",
    "                          'B3 Repair', 'B4 Replacement', 'B5 Refurbishment', 'B6 Operational energy use', \n",
    "                          'B7 Operational water use', 'C1 Deconstruction / demolition', 'C2 Transport', \n",
    "                          'C3 Waste processing', 'C4 Disposal', 'D Reuse, recovery, recycling']\n",
    "\n",
    "    \n",
    "    def clean_and_convert_to_float(series):\n",
    "        # Replace commas with periods\n",
    "        series = series.str.replace(',', '.', regex=False)\n",
    "        # Convert to numeric (float), coercing errors to NaN\n",
    "        return pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Apply the cleaning and conversion function to each column\n",
    "    combined_df[columns_to_convert] = combined_df[columns_to_convert].apply(clean_and_convert_to_float)\n",
    "\n",
    "\n",
    "    # Save the data to Excel\n",
    "    file_path = f'{output_path}/{pdf}_data.xlsx'\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "        combined_df.to_excel(writer, sheet_name='Combined Data_check', index=False)\n",
    "\n",
    "    print(f'Data successfully saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83217436-99c5-4b27-b8f7-db95d36e582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Excel/Glass1_data.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S2878088\\AppData\\Local\\Temp\\ipykernel_14364\\3972470706.py:19: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  combined_df['Date of Publication'] = pd.to_datetime(combined_df['Date of Publication'], errors='coerce', dayfirst=True)\n",
      "C:\\Users\\S2878088\\AppData\\Local\\Temp\\ipykernel_14364\\3972470706.py:20: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  combined_df['Valid Until'] = pd.to_datetime(combined_df['Valid Until'], errors='coerce', dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "# # Extract data of all available EPD from the 'EPD' directory. \n",
    "\n",
    "# # Glass (29 EPD's)\n",
    "# for i in range(1,30):\n",
    "#     pdf = f\"Glass{i}\"\n",
    "#     pdf_path = f\"EPD/{pdf}\"\n",
    "#     env_impacts_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", env_impacts_pair1, env_impacts_pair2, env_impacts_columns, env_impacts_start_keyword, env_impacts_num_columns, env_rows, standard_rows)\n",
    "#     resource_use_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", resource_use_pair1, resource_use_pair2, resource_use_columns, resource_use_start_keyword, resource_use_num_columns, resource_rows, standard_rows)\n",
    "#     waste_impacts_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", waste_pair1, waste_pair2, waste_columns, waste_start_keyword, waste_num_columns, waste_rows, standard_rows)\n",
    "#     other_details = extract_other_details_from_pdf(f\"{pdf_path}.pdf\")\n",
    "#     process_and_save_data(env_impacts_df, resource_use_df, waste_impacts_df, other_details, pdf, output_path='Excel')\n",
    "\n",
    "# # Gyproc (10 EPD's)\n",
    "# for i in range(1,12):\n",
    "#     if i == 9:\n",
    "# # This EPD is not valid\n",
    "#         continue\n",
    "#     pdf = f\"Gyproc{i}\"\n",
    "#     pdf_path = f\"EPD/{pdf}\"\n",
    "#     env_impacts_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", env_impacts_pair1, env_impacts_pair2, env_impacts_columns, env_impacts_start_keyword, env_impacts_num_columns, env_rows, standard_rows)\n",
    "#     resource_use_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", resource_use_pair1, resource_use_pair2, resource_use_columns, resource_use_start_keyword, resource_use_num_columns, resource_rows, standard_rows)\n",
    "#     waste_impacts_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", waste_pair1, waste_pair2, waste_columns, waste_start_keyword, waste_num_columns, waste_rows, standard_rows)\n",
    "#     other_details = extract_other_details_from_pdf(f\"{pdf_path}.pdf\")\n",
    "#     process_and_save_data(env_impacts_df, resource_use_df, waste_impacts_df, other_details, pdf, output_path='Excel')\n",
    "\n",
    "# # VDS (3 EPD's)\n",
    "# for i in range(1,4):\n",
    "#     pdf = f\"VDS{i}\"\n",
    "#     pdf_path = f\"EPD/{pdf}\"\n",
    "#     env_impacts_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", env_impacts_pair1, env_impacts_pair2, env_impacts_columns, env_impacts_start_keyword, env_impacts_num_columns, env_rows, standard_rows)\n",
    "#     resource_use_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", resource_use_pair1, resource_use_pair2, resource_use_columns, resource_use_start_keyword, resource_use_num_columns, resource_rows, standard_rows)\n",
    "#     waste_impacts_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", waste_pair1, waste_pair2, waste_columns, waste_start_keyword, waste_num_columns, waste_rows, standard_rows)\n",
    "#     other_details = extract_other_details_from_pdf(f\"{pdf_path}.pdf\")\n",
    "#     process_and_save_data(env_impacts_df, resource_use_df, waste_impacts_df, other_details, pdf, output_path='Excel')\n",
    "\n",
    "\n",
    "# # Provide the name of EPD in 'pdf' variable. The EPD must be stored in 'EPD' directory. \n",
    "# # Extracts all the tables and metadata and finally processes and stores it as an excel in 'Excel' directory. \n",
    "pdf = f\"Glass1\"\n",
    "pdf_path = f\"EPD/{pdf}\"\n",
    "env_impacts_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", env_impacts_pair1, env_impacts_pair2, env_impacts_columns, env_impacts_start_keyword, env_impacts_num_columns, env_rows, standard_rows)\n",
    "resource_use_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", resource_use_pair1, resource_use_pair2, resource_use_columns, resource_use_start_keyword, resource_use_num_columns, resource_rows, standard_rows)\n",
    "waste_impacts_df = extract_data_from_pdf(f\"{pdf_path}.pdf\", waste_pair1, waste_pair2, waste_columns, waste_start_keyword, waste_num_columns, waste_rows, standard_rows)\n",
    "other_details = extract_other_details_from_pdf(f\"{pdf_path}.pdf\")\n",
    "process_and_save_data(env_impacts_df, resource_use_df, waste_impacts_df, other_details, pdf, output_path='Excel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89eaecf-4010-4d5b-8578-70b29cb6b682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
